# ci tests that use a separate repository to run (can potentially share ci resources with other workflows)
name: berkeley-cluster-tests-1

on:
  # run ci when pring to main (note: ci runs on the merge commit of the pr!)
  pull_request:
    branches:
      - main
      - stable

defaults:
  run:
    shell: bash -leo pipefail {0}

env:
  # needed for local FPGA build bitstream (access GH repo to store bitstreams)
  PERSONAL_ACCESS_TOKEN: ${{ secrets.BARTENDER_PERSONAL_ACCESS_TOKEN }}

  # temporary directories should be located in /scratch (since it's larger)
  REMOTE_WORK_DIR: /scratch/buildbot/fs-shared/fs-${{ github.sha }}-${{ github.workflow }}

  # misc
  TERM: xterm-256-color

jobs:
  cancel-prior-workflows:
    name: cancel-prior-workflows
    runs-on: ubuntu-22.04
    steps:
      - name: Cancel previous workflow runs
        if: ${{ contains(github.event.pull_request.labels.*.name, 'ci:persist-prior-workflows') != true }}
        uses: styfle/cancel-workflow-action@0.12.1
        with:
          access_token: ${{ github.token }}

  start-workflow:
    name: start-workflow
    # unable to access env context in job.if thus have to put gh-a context expression directly here.
    # note that the check is using a boolean true instead of string 'true' since it's directly using
    # the expression not a variable like if checking against the env context string.
    if: ${{ contains(github.event.pull_request.labels.*.name, 'ci:disable') != true }}
    runs-on: ubuntu-22.04
    steps:
      - run: true

  # Set up a set of boolean conditions to control which branches of the CI
  # workflow will execute. This is based off the conditional job execution
  # example here: https://github.com/dorny/paths-filter#examples
  filter-jobs-on-changes:
    name: filter-jobs-on-changes
    runs-on: ubuntu-22.04
    needs: start-workflow
    # Queried by downstream jobs to determine if they should run.
    outputs:
      run-ci: ${{ steps.filter.outputs.all_count != steps.filter.outputs.non-ci-files_count }}
      both-conda-reqs-lock-modified: ${{
        ((steps.filter.outputs.conda-reqs == 'false') && (steps.filter.outputs.conda-lockfile == 'false')) ||
        ((steps.filter.outputs.conda-reqs == 'true') && (steps.filter.outputs.conda-lockfile == 'true')) }}
      run-cpp-lint: ${{ steps.filter.outputs.clang == 'true' }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            all:
              - '**'

            docs: &docs-handle
              - 'docs/**'
              - '.readthedocs.yml'

            release: &release-handle
              - '.github/workflows/release-notes.yml'
              - '.github/workflows/config/release-notes.json'

            non-ci-files:
              - *docs-handle
              - *release-handle
              - '**/*.md'
              - '**/.gitignore'
              - '**/.gitattributes'
              - '.github/ISSUE_TEMPLATE/**'
              - '.github/dependabot.yml'
              - '.mergify.yml'

            clang:
              - '.clang-*'
              - '**/*.cc'
              - '**/*.h'

            conda-reqs:
                - 'conda-reqs/*.yaml'

            conda-lockfile:
                - 'conda-reqs/*.conda-lock.yml'

  # Note: This doesn't check if the lock file is synced/faithful to the requirements file.
  # This just ensures that both were modified in the same PR (ideally the lock file was regenerated
  # from the requirements file). This job only runs when that condition is not met and
  # so always fails.
  check-conda-lock-modified:
    name: check-conda-lock-modified
    needs: filter-jobs-on-changes
    if: needs.filter-jobs-on-changes.outputs.both-conda-reqs-lock-modified == 'false'
    runs-on: ubuntu-22.04
    steps:
      - name: Check conda lock file was regenerated with conda requirements file
        run: |
          echo "ERROR: Either the conda-reqs/{firesim,ci-shared}.yaml or conda-reqs/conda-reqs.conda-lock.yml was not updated properly. See the developer docs for more information"
          false

  documentation-check:
    name: documentation-check
    runs-on: ubuntu-22.04
    needs: filter-jobs-on-changes
    steps:
      - uses: actions/checkout@v4
      - uses: conda-incubator/setup-miniconda@v3
        with:
          environment-file: conda-reqs/docs.yaml
          miniforge-version: latest
      - name: Check documentation formatting
        run: |
          make -C docs formatcheck
      - name: Check that documentation builds with selective warnings/errors
        run: |
          make -C docs html
          ! grep -v "ERROR: Undefined substitution referenced" warnings.txt
      - name: Show error log and dump objects.inv from sphinx if failed
        if: ${{ failure() }}
        run: |
          python3 -m sphinx.ext.intersphinx docs/_build/html/objects.inv
          cat /tmp/sphinx-err*.log
#does not fail anymore 
  setup-persistent-repo:
    name: setup-persistent-repo
    runs-on: local-fpga
    needs: filter-jobs-on-changes
    if: needs.filter-jobs-on-changes.outputs.run-ci == 'true'
    steps:
      # This forces a fresh clone of the repo during the `checkout` step
      # to resolve stale submodule URLs. See https://github.com/ucb-bar/chipyard/pull/1156.
      - name: Delete old checkout
        run: |
            ls -alh .
            rm -rf ${{ github.workspace }}/* || true
            rm -rf ${{ github.workspace }}/.* || true
            ls -alh .
      - uses: actions/checkout@v4
      - name: Setup repo copy
        run: |
          mkdir -p $(dirname ${{ env.REMOTE_WORK_DIR }})
          git clone ${{ github.workspace }} ${{ env.REMOTE_WORK_DIR }}
      - name: Setup repo (for xilinx_alveo_u250 platform)
        run: |
          cd ${{ env.REMOTE_WORK_DIR }}
          ./build-setup.sh --verbose
          source sourceme-manager.sh --skip-ssh-setup
          firesim managerinit --platform xilinx_alveo_u250

  cpp-lint:
    name: cpp-lint
    needs: [filter-jobs-on-changes, setup-persistent-repo]
    runs-on: local-fpga
    if: needs.filter-jobs-on-changes.outputs.run-cpp-lint == 'true'
    steps:
      - uses: actions/checkout@v4
      # TODO: disabled for now. clang-format error's with no modifications
      ## Run 'clang-format', comparing against the base commit hash.
      ## If anything got reformatted, fail and output a patch.
      ## Ensure you are using 'git' coming from conda to get 'clang-format'.
      #- name: Run clang-format
      #  run: |
      #    cd ${{ env.REMOTE_WORK_DIR }}
      #    source sourceme-manager.sh --skip-ssh-setup
      #    cd ${{ github.workspace }}
      #    git fetch --recurse-submodules=no origin ${{ github.base_ref }}
      #    DIFF_COMMIT=$(git rev-parse origin/${{ github.base_ref }})
      #    git clang-format $DIFF_COMMIT
      #    git diff --ignore-submodules > clang-format.patch
      #    if [ -s clang-format.patch ]; then
      #      echo "error: clang-format had to fix the following files:"
      #      git diff --ignore-submodules --name-only
      #      echo "----- 8< ---- PATCH ----- 8< ----"
      #      cat clang-format.patch
      #      echo "----- 8< ---- PATCH ----- 8< ----"
      #      git checkout .
      #      exit 1
      #    fi
      # TODO: disabled for now. clang-tidy is throwing alot of false positives
      ## Run clang-tidy on the entire codebase. Error will be logged.
      #- name: Run clang-tidy
      #  run: ./.github/scripts/invoke-make.py "clang-tidy"

  run-scala-lint:
    name: run-scala-lint
    needs: [setup-persistent-repo]
    runs-on: local-fpga
    steps:
      - uses: actions/checkout@v4
      - name: Scala fix + format check
        run: ./.github/scripts/invoke-make.py "scala-lint-check"

  run-python-lint:
    name: run-python-lint
    needs: [setup-persistent-repo]
    runs-on: local-fpga
    steps:
      - uses: actions/checkout@v4
      - name: Manager Python formatting check using 'black'
        run: ./.github/scripts/invoke-script.py "./scripts/run-manager-python-reformat.sh --check"
      - name: CI Python typecheck
        run: ./.github/scripts/invoke-script.py "./scripts/run-ci-python-typecheck.sh"
      - name: Manager Python typecheck
        run: ./.github/scripts/invoke-script.py "./scripts/run-manager-python-typecheck.sh"

  check-docs-generated-components-xilinx_alveo_u250:
    name: check-docs-generated-components-xilinx_alveo_u250
    needs: [setup-persistent-repo]
    runs-on: local-fpga
    steps:
      - uses: actions/checkout@v4
      - name: Check docs components that require manual re-generation (e.g. config_runtime.yaml example)
        run: ./.github/scripts/check-docs-generated-components.py --platform xilinx_alveo_u250

  build-driver-xilinx_alveo_u250:
    name: build-driver-xilinx_alveo_u250
    needs: [setup-persistent-repo]
    runs-on: local-fpga
    steps:
      - uses: actions/checkout@v4
      - name: Build driver
        run: .github/scripts/build-driver.py --platform xilinx_alveo_u250

  cleanup-local-fpga-repo:
    name: cleanup-local-fpga-repo
    needs: [
      run-python-lint,
      run-scala-lint,
      check-docs-generated-components-xilinx_alveo_u250,
      build-driver-xilinx_alveo_u250]
    # uses a separate runner to cleanup (irrespective, of other jobs cancelled, running, etc)
    runs-on: local-fpga-cleanup
    if: ${{ always() }}
    steps:
      - name: Delete repo copy
        run: rm -rf ${{ env.REMOTE_WORK_DIR }}
